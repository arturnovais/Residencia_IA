{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4ff6916",
   "metadata": {},
   "source": [
    "# A Neural Probabilistic Language Model\n",
    "\n",
    "Nesse notebook temos como objetivo replicar a ideia do artigo \"A Neural Probabilistic Language Model\". A classe de treinmaneot do language model surge do medium: [A Neural Probabilistic Language Model: Breaking Down Bengio’s Approach](https://medium.com/@dahami/a-neural-probabilistic-language-model-breaking-down-bengios-approach-4bf793a84426)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2859ac55",
   "metadata": {},
   "source": [
    "## Dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cadb9a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "import random\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73ab933",
   "metadata": {},
   "source": [
    "## Classe do Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e2b32c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralProbabilisticLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size, hidden_dim):\n",
    "        super(NeuralProbabilisticLanguageModel, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)  \n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, hidden_dim)\n",
    "        self.tanh = nn.Tanh()  \n",
    "        self.linear2 = nn.Linear(hidden_dim, vocab_size)  \n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        emb = self.embeddings(inputs)            \n",
    "        if emb.dim() == 2:                         \n",
    "            emb = emb.unsqueeze(0)  \n",
    "                        \n",
    "        out = emb.flatten(start_dim=1)           \n",
    "        out = self.tanh(self.linear1(out))\n",
    "        out = self.linear2(out)\n",
    "        \n",
    "        return torch.log_softmax(out, dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c0f41d",
   "metadata": {},
   "source": [
    "## Carregando dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4a5e8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manter_letras_e_espacos(texto: str) -> str:\n",
    "    if texto is None:\n",
    "        return \"\"\n",
    "    s = str(texto).lower()\n",
    "    return re.sub(r\"[^a-z ]+\", \"\", s)\n",
    "\n",
    "def adicionar_coluna_normalizada(exemplo):\n",
    "    exemplo[\"texto_normalizado\"] = manter_letras_e_espacos(exemplo.get(\"text\", \"\"))\n",
    "    return exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "13dad83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_hf = \"books\"\n",
    "split = \"train\"\n",
    "ds = load_dataset(\"ubaada/booksum-complete-cleaned\", config_hf, split=split)\n",
    "ds = ds.map(adicionar_coluna_normalizada)\n",
    "ds = ds.filter(lambda x: len(x[\"texto_normalizado\"].strip()) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ade20a",
   "metadata": {},
   "source": [
    "## Utilitários do treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b1783a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando o dispositivo: mps\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 5      \n",
    "EMB_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "MAX_VOCAB = 40000         \n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 1\n",
    "LR = 3e-5\n",
    "SEED = 42\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available() else \"cpu\"))\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "\n",
    "print(f\"Usando o dispositivo: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "860b6a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return re.sub(r\"\\s+\", \" \", text.strip()).split(\" \")\n",
    "\n",
    "ds_split = ds.train_test_split(test_size=0.01, seed=SEED)\n",
    "train_texts = ds_split[\"train\"][\"texto_normalizado\"]\n",
    "valid_texts = ds_split[\"test\"][\"texto_normalizado\"]\n",
    "\n",
    "SPECIALS = [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"]\n",
    "counter = Counter()\n",
    "for t in train_texts:\n",
    "    counter.update(tokenize(t))\n",
    "\n",
    "most_common = [w for w, _ in counter.most_common(MAX_VOCAB - len(SPECIALS))]\n",
    "itos = SPECIALS + most_common\n",
    "stoi = {w: i for i, w in enumerate(itos)}\n",
    "\n",
    "PAD, UNK, BOS, EOS = (stoi[\"<pad>\"], stoi[\"<unk>\"], stoi[\"<bos>\"], stoi[\"<eos>\"])\n",
    "\n",
    "def encode(tokens):\n",
    "    return [stoi.get(w, UNK) for w in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9474ee3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_windows_from_text(text, context_size=CONTEXT_SIZE):\n",
    "    toks = tokenize(text)\n",
    "    toks = [\"<bos>\"] * context_size + toks + [\"<eos>\"]\n",
    "    ids = encode(toks)\n",
    "    contexts, targets = [], []\n",
    "    for i in range(context_size, len(ids)):\n",
    "        contexts.append(ids[i - context_size:i])\n",
    "        targets.append(ids[i])\n",
    "    return contexts, targets\n",
    "\n",
    "class NGramDataset(Dataset):\n",
    "    def __init__(self, texts, context_size):\n",
    "        self.contexts = []\n",
    "        self.targets = []\n",
    "        for text in texts:\n",
    "            c, y = make_windows_from_text(text, context_size)\n",
    "            self.contexts.extend(c)\n",
    "            self.targets.extend(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.contexts[idx], dtype=torch.long), torch.tensor(self.targets[idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "95a2040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = NGramDataset(train_texts, CONTEXT_SIZE)\n",
    "valid_ds = NGramDataset(valid_texts, CONTEXT_SIZE)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "79ec76d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralProbabilisticLanguageModel(\n",
    "    vocab_size=len(itos),\n",
    "    embedding_dim=EMB_DIM,\n",
    "    context_size=CONTEXT_SIZE,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    ").to(DEVICE)\n",
    "\n",
    "criterion = nn.NLLLoss(ignore_index=PAD)  \n",
    "optim = torch.optim.AdamW(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bfe068",
   "metadata": {},
   "source": [
    "## Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "640c5796",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_EVERY = 30\n",
    "\n",
    "GOOD_SEEDS = [\n",
    "    \"the king of france\",\n",
    "    \"once upon a time\",\n",
    "    \"in a distant land\",\n",
    "    \"deep learning models\",\n",
    "    \"the history of science\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "162c1251",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_from_seed(seed_text, max_new_tokens=30, temperature=0.9, top_k=50):\n",
    "    model.eval()\n",
    "    seed_words = seed_text.lower().split()\n",
    "\n",
    "    ctx = [BOS] * CONTEXT_SIZE\n",
    "    seed_ids = encode(seed_words[-CONTEXT_SIZE:])\n",
    "    for i, sid in enumerate(seed_ids[::-1]):\n",
    "        ctx[-1 - i] = sid\n",
    "\n",
    "    out_tokens = seed_words[:]\n",
    "    for _ in range(max_new_tokens):\n",
    "        x = torch.tensor(ctx, dtype=torch.long, device=DEVICE)    \n",
    "        logp = model(x)                                            \n",
    "        logits = logp[0] * temperature\n",
    "\n",
    "        if top_k and top_k < logits.numel():\n",
    "            kth = torch.topk(logits, top_k).values[-1]\n",
    "            logits = torch.where(logits < kth, torch.tensor(float('-inf'), device=logits.device), logits)\n",
    "\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, 1).item()\n",
    "        tok = itos[next_id]\n",
    "        if tok == \"<eos>\":\n",
    "            break\n",
    "        if tok not in SPECIALS:\n",
    "            out_tokens.append(tok)\n",
    "        ctx = ctx[1:] + [next_id]\n",
    "    return \" \".join(out_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ae188179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(dataloader, train=True, log_every=LOG_EVERY):\n",
    "    model.train(train)\n",
    "    total_loss, total_items = 0.0, 0\n",
    "    pbar = tqdm(enumerate(dataloader, start=1), total=len(dataloader),\n",
    "                desc=f\"{'Train' if train else 'Valid'}\", leave=False)\n",
    "\n",
    "    for step, (x_batch, y_batch) in pbar:\n",
    "        x_batch, y_batch = x_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "        bs = x_batch.size(0)\n",
    "\n",
    "        if train:\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "\n",
    "        batch_loss_val = 0.0\n",
    "\n",
    "        for i in range(bs):\n",
    "            log_probs = model(x_batch[i])                          \n",
    "            loss_i = criterion(log_probs, y_batch[i].unsqueeze(0))\n",
    "            if train:\n",
    "                (loss_i / bs).backward()\n",
    "            batch_loss_val += loss_i.item()\n",
    "\n",
    "        if train:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optim.step()\n",
    "\n",
    "        total_loss += batch_loss_val\n",
    "        total_items += bs\n",
    "        avg = total_loss / max(1, total_items)\n",
    "        pbar.set_postfix(avg_loss=f\"{avg:.4f}\")\n",
    "\n",
    "        if train and (step % log_every == 0):\n",
    "            ppl = math.exp(min(20, avg))\n",
    "            tqdm.write(f\"step {step}/{len(dataloader)} | avg_loss={avg:.4f} | ppl={ppl:.2f}\")\n",
    "            seed = random.choice(GOOD_SEEDS)\n",
    "            sample = generate_from_seed(seed, max_new_tokens=25, temperature=0.9, top_k=50)\n",
    "            tqdm.write(f\"▶️ seed: '{seed}'\\n📝 {sample}\\n\")\n",
    "            if hasattr(torch, \"mps\") and torch.backends.mps.is_available():\n",
    "                torch.mps.synchronize()\n",
    "\n",
    "    avg = total_loss / max(1, total_items)\n",
    "    ppl = math.exp(min(20, avg))\n",
    "    return avg, ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ef9096a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶️ seed: 'the king of france'\n",
      "📝 the king of france garonne stevie effects fretting silencei drives veal themif persuasions wronged whyi isone jaded abased onlyone herinto power headman ploughing fagin andone stubble palings hedda partisan\n",
      "\n",
      "▶️ seed: 'once upon a time'\n",
      "📝 once upon a time lookthat gaywith rugs prophet yousergius stirring waywith ofduty olivers brag smokethe andwere dying demeanor chiefest fawned chantilly permanent hymn honoured brimstone disaster prospect frames thegods\n",
      "\n",
      "▶️ seed: 'in a distant land'\n",
      "📝 in a distant land mumbled catesby louise blackhair obligation asif siege sprinkled notno cords bumper crank thank corps notsee devisd mas notyour theinconvenience joked evelyn captainand forests appease locusts\n",
      "\n",
      "▶️ seed: 'deep learning models'\n",
      "📝 deep learning models longin nowmr observd wys iunderstood startingpoint angers guileless gondolas pact looming thesecond beconsidered herabout heremrs carve mountainside procurators togetherto myselfand pleasantness dorsets returnthe bobbing ofhearing\n",
      "\n",
      "▶️ seed: 'the history of science'\n",
      "📝 the history of science parks invade uponsome arow anoint tramped fragmentary everyones thatpoint purposethe petty espouse nede churl beginningto purple isand menaces attains intent hereoh bustling powerful herpresence doors\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seed in GOOD_SEEDS:\n",
    "    sample = generate_from_seed(seed, max_new_tokens=25, temperature=0.9, top_k=50)\n",
    "    tqdm.write(f\"▶️ seed: '{seed}'\\n📝 {sample}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "014684d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6de99ece93b4fd9914a033c682c22a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/23208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 30/23208 | avg_loss=10.6161 | ppl=40785.67\n",
      "▶️ seed: 'the king of france'\n",
      "📝 the king of france chronicle clarendon taxcollector andlost shecared crave norany apower habitable frilled allowedto tooif bide crier din lovemaster turnedback tennyson husbandry tightened single mlle parallels robe optical\n",
      "\n",
      "step 60/23208 | avg_loss=10.6036 | ppl=40280.39\n",
      "▶️ seed: 'the king of france'\n",
      "📝 the king of france sceneof wen gruffly levelled rendered carriageand jewelled chimed crowning tought showthe dinewith halffinished slave boars whichwere tact loveshe quelled onthen goodwill toyour dock january answer\n",
      "\n",
      "step 90/23208 | avg_loss=10.5904 | ppl=39749.79\n",
      "▶️ seed: 'in a distant land'\n",
      "📝 in a distant land associate luxuriant expelled mute truthit thenketh churning jess proclaimed reminds nicholas creative awaynow assigned adieux belles psalms uttermost ruth cottonville whenall growed meditatively inclose givet\n",
      "\n",
      "step 120/23208 | avg_loss=10.5768 | ppl=39216.33\n",
      "▶️ seed: 'once upon a time'\n",
      "📝 once upon a time lawing corporeal obscurd changei unsubstantial thoughtswere seisd crunching itsnot string thenthe becausethey manyother ferociously guinevere comesto thatand histories jane stumble bottomless catalogue umpire othershe scatterd\n",
      "\n",
      "step 150/23208 | avg_loss=10.5622 | ppl=38646.55\n",
      "▶️ seed: 'once upon a time'\n",
      "📝 once upon a time glances spells peals dorrit steaks wasabsolutely faults delawares rightyes spontaneous clumsiness chopper pleye majesty tortoiseshell moonlit aclose outlined compensated thepavement illustrates thosewhich curious shetold exuberance\n",
      "\n",
      "step 180/23208 | avg_loss=10.5437 | ppl=37936.06\n",
      "▶️ seed: 'once upon a time'\n",
      "📝 once upon a time hearit dover safe youare wemay sir triumph stranger attends amistake secular mum thessalonica isaid youmy polish aids strengthening need esteemed atime convictions sneak casket handof\n",
      "\n",
      "step 210/23208 | avg_loss=10.5228 | ppl=37153.21\n",
      "▶️ seed: 'the king of france'\n",
      "📝 the king of france interment horseshoe yed smoothly shooting discordant those sustaind thefederal allwith speciesof bazulto imagining zeeco thebar mindful forthey sweetheart inconsistencies atthe fat bustling thought benecessary gloue\n",
      "\n",
      "step 240/23208 | avg_loss=10.4986 | ppl=36266.24\n",
      "▶️ seed: 'the history of science'\n",
      "📝 the history of science rote dislike momentof myaunt caleb jessica theyfelt dexterous countenances movie halfclosed premise hiding low target hadfinished herit protective marks gideon repliedwith yard opportunity fact plodding\n",
      "\n",
      "step 270/23208 | avg_loss=10.4712 | ppl=35286.04\n",
      "▶️ seed: 'the king of france'\n",
      "📝 the king of france five ceased bridgnorth pines nowill resembled charities bunch theletters light herconscience heat du overwhelming official andhelp constantine therto helpeth darkblue drum terminating everso withinterest glossy\n",
      "\n",
      "step 300/23208 | avg_loss=10.4398 | ppl=34192.26\n",
      "▶️ seed: 'the history of science'\n",
      "📝 the history of science the when been radiant andfind athelstane retainers bellboy might fogs baltimore toit chant mightst youa dearshe palestine lotuses ordain wentthe blithely antiquated thedust player leeches\n",
      "\n",
      "step 330/23208 | avg_loss=10.4063 | ppl=33066.09\n",
      "▶️ seed: 'deep learning models'\n",
      "📝 deep learning models flora the actually families acrowd ofjustice century begone vow dorothea fightand im calendar peculiarity raspberry wouldnt critic graces marcius hove towait cravats observedthat donethere herthe\n",
      "\n",
      "step 360/23208 | avg_loss=10.3690 | ppl=31856.78\n",
      "▶️ seed: 'the king of france'\n",
      "📝 the king of france an of note so carlo exultation allowedto fixes yoked preparedfor haystack wasof forcd davy iust waits ofgold have studs acquaintance occasion path collegians goodlooking broad\n",
      "\n",
      "step 390/23208 | avg_loss=10.3271 | ppl=30548.82\n",
      "▶️ seed: 'the king of france'\n",
      "📝 the king of france governments who so operations ruthless disconcerting payments itwhats oftheir peaceably menhave upthe placethen asister profit exhaustion onsuch sociable frontdoor theyn merrymaking project shyness spine and\n",
      "\n",
      "step 420/23208 | avg_loss=10.2819 | ppl=29200.44\n",
      "▶️ seed: 'the king of france'\n",
      "📝 the king of france was he the and doubt a population bedclothes aside default aware visible burden satans husbands haveyou nowas it limbs saidthe gods attack features didnot cease\n",
      "\n",
      "step 450/23208 | avg_loss=10.2354 | ppl=27871.79\n",
      "▶️ seed: 'once upon a time'\n",
      "📝 once upon a time for be woodman in with did queer when thy when police go dreamers handiwork swooped meoh recurring himsome pianoforte unsafe sperrit doubt thenoh\n",
      "\n",
      "step 480/23208 | avg_loss=10.1846 | ppl=26490.86\n",
      "▶️ seed: 'once upon a time'\n",
      "📝 once upon a time iam his asserted languages with home askedno pitching summer hecrossed dorrit stigmatized surfaceof straggled spoil likeit andreturned agony dilling thatthose irresolution embark thefeet takenthe humdrum\n",
      "\n",
      "step 510/23208 | avg_loss=10.1325 | ppl=25146.36\n",
      "▶️ seed: 'the history of science'\n",
      "📝 the history of science im the did though the england which you are walked isas discomposure threatening the so dandak at the lady when his\n",
      "\n",
      "step 540/23208 | avg_loss=10.0776 | ppl=23802.67\n",
      "▶️ seed: 'the history of science'\n",
      "📝 the history of science in the up the that in before in very they of the home and there to his a\n",
      "\n",
      "step 570/23208 | avg_loss=10.0205 | ppl=22483.75\n",
      "▶️ seed: 'the king of france'\n",
      "📝 the king of france big who in a at very the in he a are the the have in of the\n",
      "\n",
      "step 600/23208 | avg_loss=9.9620 | ppl=21204.88\n",
      "▶️ seed: 'the history of science'\n",
      "📝 the history of science i the said with when that the the have i of\n",
      "\n",
      "step 630/23208 | avg_loss=9.9028 | ppl=19985.73\n",
      "▶️ seed: 'once upon a time'\n",
      "📝 once upon a time before to the the of the a of he said\n",
      "\n",
      "step 660/23208 | avg_loss=9.8416 | ppl=18799.85\n",
      "▶️ seed: 'the history of science'\n",
      "📝 the history of science the so are thou so be loans mysituation halted dismissd ransome pat a she that back should these but there the the you\n",
      "\n",
      "step 690/23208 | avg_loss=9.7801 | ppl=17677.83\n",
      "▶️ seed: 'deep learning models'\n",
      "📝 deep learning models these a a the the the of the the the the\n",
      "\n",
      "step 720/23208 | avg_loss=9.7200 | ppl=16647.54\n",
      "▶️ seed: 'once upon a time'\n",
      "📝 once upon a time than not she were a said the the the the a the you when the the the\n",
      "\n",
      "step 750/23208 | avg_loss=9.6607 | ppl=15688.76\n",
      "▶️ seed: 'deep learning models'\n",
      "📝 deep learning models of the and a a it was for a the the the of the the the he\n",
      "\n",
      "step 780/23208 | avg_loss=9.6002 | ppl=14768.00\n",
      "▶️ seed: 'the history of science'\n",
      "📝 the history of science as so how to him at the the the a of the had of she to a\n",
      "\n",
      "step 810/23208 | avg_loss=9.5415 | ppl=13925.61\n",
      "▶️ seed: 'in a distant land'\n",
      "📝 in a distant land his this his had was the and the for and the of the\n",
      "\n",
      "step 840/23208 | avg_loss=9.4841 | ppl=13148.95\n",
      "▶️ seed: 'the king of france'\n",
      "📝 the king of france which was i a a be he which the his the and the of a the who\n",
      "\n",
      "step 870/23208 | avg_loss=9.4279 | ppl=12430.77\n",
      "▶️ seed: 'once upon a time'\n",
      "📝 once upon a time i of the a of there that at had he what he the was i of and what\n",
      "\n",
      "step 900/23208 | avg_loss=9.3726 | ppl=11761.26\n",
      "▶️ seed: 'deep learning models'\n",
      "📝 deep learning models that it this which had the who it said in with is they said of of at not of the\n",
      "\n",
      "step 930/23208 | avg_loss=9.3193 | ppl=11151.10\n",
      "▶️ seed: 'in a distant land'\n",
      "📝 in a distant land her of the the to the and and not and which you a who for the i the the\n",
      "\n",
      "step 960/23208 | avg_loss=9.2687 | ppl=10600.66\n",
      "▶️ seed: 'in a distant land'\n",
      "📝 in a distant land i the i a of to and the in my the his to the would had and\n",
      "\n",
      "step 990/23208 | avg_loss=9.2185 | ppl=10081.73\n",
      "▶️ seed: 'once upon a time'\n",
      "📝 once upon a time he was had to when and had and her to as a a i that her you it\n",
      "\n",
      "step 1020/23208 | avg_loss=9.1703 | ppl=9607.93\n",
      "▶️ seed: 'once upon a time'\n",
      "📝 once upon a time of not the so it that of and a at the so of there the when of and he\n",
      "\n",
      "step 1050/23208 | avg_loss=9.1227 | ppl=9160.46\n",
      "▶️ seed: 'in a distant land'\n",
      "📝 in a distant land her of to my the which you her of to she what the the and i a to his\n",
      "\n",
      "step 1080/23208 | avg_loss=9.0755 | ppl=8738.56\n",
      "▶️ seed: 'the king of france'\n",
      "📝 the king of france of and and that her to a her a what me to to them his my that the the who to a a\n",
      "\n",
      "step 1110/23208 | avg_loss=9.0331 | ppl=8375.51\n",
      "▶️ seed: 'the king of france'\n",
      "📝 the king of france you his and the the you a in with in it a was to i i he as his\n",
      "\n",
      "step 1140/23208 | avg_loss=8.9891 | ppl=8015.52\n",
      "▶️ seed: 'deep learning models'\n",
      "📝 deep learning models said of the in but the this if her and his i he the the i but a his and a he from of\n",
      "\n",
      "step 1170/23208 | avg_loss=8.9472 | ppl=7686.55\n",
      "▶️ seed: 'the king of france'\n",
      "📝 the king of france his of my for of at and the of as in there for at as but when to at at your are you in a\n",
      "\n",
      "step 1200/23208 | avg_loss=8.9075 | ppl=7387.47\n",
      "▶️ seed: 'in a distant land'\n",
      "📝 in a distant land that the by was his so no were in of in and the a and with to the my of at was she\n",
      "\n",
      "step 1230/23208 | avg_loss=8.8676 | ppl=7098.16\n",
      "▶️ seed: 'in a distant land'\n",
      "📝 in a distant land who in have one that it so of there there and had a all said all would if by to you\n",
      "\n",
      "step 1260/23208 | avg_loss=8.8309 | ppl=6842.72\n",
      "▶️ seed: 'the history of science'\n",
      "📝 the history of science his and were she so with a that that had it said all your there but to this that i the and i\n",
      "\n",
      "step 1290/23208 | avg_loss=8.7946 | ppl=6598.56\n",
      "▶️ seed: 'in a distant land'\n",
      "📝 in a distant land her i as to no who and a she that by and so it of the and him to which that\n",
      "\n",
      "step 1320/23208 | avg_loss=8.7601 | ppl=6374.47\n",
      "▶️ seed: 'the king of france'\n",
      "📝 the king of france are is a she a that if the and the was and for is the a me i is the by of\n",
      "\n",
      "step 1350/23208 | avg_loss=8.7269 | ppl=6166.82\n",
      "▶️ seed: 'deep learning models'\n",
      "📝 deep learning models who it their his by of the for when to his the be in me at and of no an that not for have\n",
      "\n",
      "step 1380/23208 | avg_loss=8.6949 | ppl=5972.54\n",
      "▶️ seed: 'the history of science'\n",
      "📝 the history of science that i to at her have and the this said the would and not was his are you i no\n",
      "\n",
      "step 1410/23208 | avg_loss=8.6628 | ppl=5783.97\n",
      "▶️ seed: 'the king of france'\n",
      "📝 the king of france to are for my with had the it if as the the her the his of the was have no a with with\n",
      "\n",
      "step 1440/23208 | avg_loss=8.6334 | ppl=5616.37\n",
      "▶️ seed: 'deep learning models'\n",
      "📝 deep learning models for not my but no a to that for of there he he to in of is you that but\n",
      "\n",
      "step 1470/23208 | avg_loss=8.6041 | ppl=5454.23\n",
      "▶️ seed: 'the king of france'\n",
      "📝 the king of france is the but and had you so to his he at at the of of there to you his at so for\n",
      "\n",
      "step 1500/23208 | avg_loss=8.5754 | ppl=5299.46\n",
      "▶️ seed: 'the history of science'\n",
      "📝 the history of science his her i a a i had all the his a what and you my what they on\n",
      "\n",
      "step 1530/23208 | avg_loss=8.5471 | ppl=5151.65\n",
      "▶️ seed: 'in a distant land'\n",
      "📝 in a distant land and and he a was from all were in had this the i could to and had the he by the\n",
      "\n",
      "step 1560/23208 | avg_loss=8.5204 | ppl=5016.27\n",
      "▶️ seed: 'the history of science'\n",
      "📝 the history of science which the have my was to to her she to to had and had be in the was with that the have\n",
      "\n",
      "step 1590/23208 | avg_loss=8.4944 | ppl=4887.26\n",
      "▶️ seed: 'in a distant land'\n",
      "📝 in a distant land of this all the they not i i for when with to my as and you he the it in were her the\n",
      "\n",
      "step 1620/23208 | avg_loss=8.4682 | ppl=4761.12\n",
      "▶️ seed: 'the history of science'\n",
      "📝 the history of science and in with the or was the in and my were would and the when of the their in it this she\n",
      "\n",
      "step 1650/23208 | avg_loss=8.4433 | ppl=4643.73\n",
      "▶️ seed: 'once upon a time'\n",
      "📝 once upon a time or i the of the who for his an the you is what me or not but that some her a\n",
      "\n",
      "step 1680/23208 | avg_loss=8.4193 | ppl=4533.64\n",
      "▶️ seed: 'the king of france'\n",
      "📝 the king of france which her on in he what a or that said the him of the is it were for with a the with\n",
      "\n",
      "step 1710/23208 | avg_loss=8.3964 | ppl=4431.06\n",
      "▶️ seed: 'the king of france'\n",
      "📝 the king of france the on with was you a by for they i of the this his him that or i is it\n",
      "\n",
      "step 1740/23208 | avg_loss=8.3741 | ppl=4333.23\n",
      "▶️ seed: 'once upon a time'\n",
      "📝 once upon a time the you you my are as not i that which on their that you of in the no of my were with\n",
      "\n",
      "step 1770/23208 | avg_loss=8.3518 | ppl=4237.60\n",
      "▶️ seed: 'in a distant land'\n",
      "📝 in a distant land he the she have he it i was the have on the there as the you and of on\n",
      "\n",
      "step 1800/23208 | avg_loss=8.3305 | ppl=4148.52\n",
      "▶️ seed: 'the king of france'\n",
      "📝 the king of france of i in and the of to her for of of me the for her and were with at\n",
      "\n",
      "step 1830/23208 | avg_loss=8.3105 | ppl=4066.42\n",
      "▶️ seed: 'once upon a time'\n",
      "📝 once upon a time could all it which if my in him of a had it are to i the that a had what in the\n",
      "\n",
      "step 1860/23208 | avg_loss=8.2901 | ppl=3984.16\n",
      "▶️ seed: 'the king of france'\n",
      "📝 the king of france be the of of the and of or one was me with by the you by had in me for\n",
      "\n",
      "step 1890/23208 | avg_loss=8.2700 | ppl=3904.95\n",
      "▶️ seed: 'deep learning models'\n",
      "📝 deep learning models he i the do it her which and in not was his to from have which was had no me have\n",
      "\n",
      "step 1920/23208 | avg_loss=8.2507 | ppl=3830.41\n",
      "▶️ seed: 'in a distant land'\n",
      "📝 in a distant land as to i a if the on of was was what to in if it i from had that had his been and\n",
      "\n",
      "step 1950/23208 | avg_loss=8.2322 | ppl=3760.24\n",
      "▶️ seed: 'deep learning models'\n",
      "📝 deep learning models in of was the him you a one she a you which and for be in he that a at from and\n",
      "\n",
      "step 1980/23208 | avg_loss=8.2140 | ppl=3692.12\n",
      "▶️ seed: 'in a distant land'\n",
      "📝 in a distant land i his a to of one her of was of he not that said that was said all in of you his in of\n",
      "\n",
      "step 2010/23208 | avg_loss=8.1966 | ppl=3628.50\n",
      "▶️ seed: 'once upon a time'\n",
      "📝 once upon a time when this of it was that to but had the she her and and the that in of and of were\n",
      "\n",
      "step 2040/23208 | avg_loss=8.1795 | ppl=3567.16\n",
      "▶️ seed: 'in a distant land'\n",
      "📝 in a distant land the to be it in but of he were it and by had be at what that it the her you be\n",
      "\n",
      "step 2070/23208 | avg_loss=8.1626 | ppl=3507.32\n",
      "▶️ seed: 'in a distant land'\n",
      "📝 in a distant land if the and in with her and of a in and no for the the been an for for to she the\n",
      "\n",
      "step 2100/23208 | avg_loss=8.1461 | ppl=3450.03\n",
      "▶️ seed: 'once upon a time'\n",
      "📝 once upon a time for the and a was of be to i the in the to by was and his to on a they\n",
      "\n",
      "step 2130/23208 | avg_loss=8.1303 | ppl=3395.94\n",
      "▶️ seed: 'in a distant land'\n",
      "📝 in a distant land the are she with me had were from a what all that the and as with and was a in\n",
      "\n",
      "step 2160/23208 | avg_loss=8.1147 | ppl=3343.09\n",
      "▶️ seed: 'the king of france'\n",
      "📝 the king of france and then was of was was for be the the of this as they a no of the it a in the have at\n",
      "\n",
      "step 2190/23208 | avg_loss=8.0997 | ppl=3293.53\n",
      "▶️ seed: 'the history of science'\n",
      "📝 the history of science of the is i a the but of would i and him of it with she of me she i that would an\n",
      "\n",
      "step 2220/23208 | avg_loss=8.0846 | ppl=3244.23\n",
      "▶️ seed: 'once upon a time'\n",
      "📝 once upon a time as there to the were be that he said this of it he from to have that the\n",
      "\n",
      "step 2250/23208 | avg_loss=8.0702 | ppl=3197.89\n",
      "▶️ seed: 'the history of science'\n",
      "📝 the history of science for he a to the his were him is it my he they not were all have his i as been\n",
      "\n",
      "step 2280/23208 | avg_loss=8.0565 | ppl=3154.17\n",
      "▶️ seed: 'once upon a time'\n",
      "📝 once upon a time but to he been not was they of for in so all will is he were are so of the not\n",
      "\n",
      "step 2310/23208 | avg_loss=8.0420 | ppl=3108.91\n",
      "▶️ seed: 'once upon a time'\n",
      "📝 once upon a time what and in and so i and and the him with but and on that when of was to the were\n",
      "\n",
      "step 2340/23208 | avg_loss=8.0281 | ppl=3066.05\n",
      "▶️ seed: 'deep learning models'\n",
      "📝 deep learning models to the and of i an to what which as and not for his me the if all i she there to\n",
      "\n",
      "step 2370/23208 | avg_loss=8.0146 | ppl=3024.71\n",
      "▶️ seed: 'deep learning models'\n",
      "📝 deep learning models he he not and to i not for no the so and a in him in he to she that which of the he this\n",
      "\n",
      "step 2400/23208 | avg_loss=8.0014 | ppl=2985.26\n",
      "▶️ seed: 'in a distant land'\n",
      "📝 in a distant land she was to to and he the to by on that if his in that and i that to the i be a\n",
      "\n",
      "step 2430/23208 | avg_loss=7.9887 | ppl=2947.51\n",
      "▶️ seed: 'the history of science'\n",
      "📝 the history of science of the the with him all at it to so that you her in her was were but on\n",
      "\n",
      "step 2460/23208 | avg_loss=7.9763 | ppl=2911.27\n",
      "▶️ seed: 'once upon a time'\n",
      "📝 once upon a time and on for of was is it what on the to at a of but the is the is\n",
      "\n",
      "step 2490/23208 | avg_loss=7.9634 | ppl=2873.96\n",
      "▶️ seed: 'in a distant land'\n",
      "📝 in a distant land they the could a of but he it are not my his on were he the but to to\n",
      "\n",
      "step 2520/23208 | avg_loss=7.9513 | ppl=2839.34\n",
      "▶️ seed: 'the king of france'\n",
      "📝 the king of france to not if the the an the of the to of the that and as and not he the and you\n",
      "\n",
      "step 2550/23208 | avg_loss=7.9394 | ppl=2805.72\n",
      "▶️ seed: 'once upon a time'\n",
      "📝 once upon a time and his him of had of had to and be have for as and if he were him as there his so he\n",
      "\n",
      "step 2580/23208 | avg_loss=7.9276 | ppl=2772.64\n",
      "▶️ seed: 'the king of france'\n",
      "📝 the king of france when all as was and not to me and the and the for so was be my are and which in the\n",
      "\n",
      "step 2610/23208 | avg_loss=7.9158 | ppl=2740.32\n",
      "▶️ seed: 'in a distant land'\n",
      "📝 in a distant land if the from the and i she on his and was i not are she the what be the\n",
      "\n",
      "step 2640/23208 | avg_loss=7.9046 | ppl=2709.78\n",
      "▶️ seed: 'deep learning models'\n",
      "📝 deep learning models said not he them the i with the my for as but what was was that you of not she to\n",
      "\n",
      "step 2670/23208 | avg_loss=7.8936 | ppl=2680.11\n",
      "▶️ seed: 'in a distant land'\n",
      "📝 in a distant land a and the was of you but one to said as at have that it what the and the\n",
      "\n",
      "step 2700/23208 | avg_loss=7.8825 | ppl=2650.57\n",
      "▶️ seed: 'the king of france'\n",
      "📝 the king of france with he were the and so him was in as a he when and and was i in and you all\n",
      "\n",
      "step 2730/23208 | avg_loss=7.8715 | ppl=2621.63\n",
      "▶️ seed: 'once upon a time'\n",
      "📝 once upon a time his to her if on you of is one and her and be in in in and was would all been were\n",
      "\n",
      "step 2760/23208 | avg_loss=7.8610 | ppl=2594.00\n",
      "▶️ seed: 'the history of science'\n",
      "📝 the history of science of a all of with him with my this for on the no his to the as you you\n",
      "\n",
      "step 2790/23208 | avg_loss=7.8505 | ppl=2567.12\n",
      "▶️ seed: 'in a distant land'\n",
      "📝 in a distant land you and i as a what not his this so and to all was the in the were to the when but the\n",
      "\n",
      "step 2820/23208 | avg_loss=7.8405 | ppl=2541.53\n",
      "▶️ seed: 'once upon a time'\n",
      "📝 once upon a time to this to of by the in but at she in the there that this and on he to the\n",
      "\n",
      "step 2850/23208 | avg_loss=7.8307 | ppl=2516.58\n",
      "▶️ seed: 'deep learning models'\n",
      "📝 deep learning models to of the the it that and which a in of the a for that that or which it and i\n",
      "\n",
      "step 2880/23208 | avg_loss=7.8205 | ppl=2491.04\n",
      "▶️ seed: 'deep learning models'\n",
      "📝 deep learning models have and him to her said she by with in his when to me to my be\n",
      "\n",
      "step 2910/23208 | avg_loss=7.8108 | ppl=2467.18\n",
      "▶️ seed: 'deep learning models'\n",
      "📝 deep learning models was her all a so you he the all the him of and the and you for you him the in at\n",
      "\n",
      "step 2940/23208 | avg_loss=7.8008 | ppl=2442.62\n",
      "▶️ seed: 'once upon a time'\n",
      "📝 once upon a time but it to him i you for so and this his for your you of that for me to one you\n",
      "\n",
      "step 2970/23208 | avg_loss=7.7915 | ppl=2419.90\n",
      "▶️ seed: 'in a distant land'\n",
      "📝 in a distant land but you this a for her would and that in have if so with the of the in what in in\n",
      "\n",
      "step 3000/23208 | avg_loss=7.7822 | ppl=2397.47\n",
      "▶️ seed: 'once upon a time'\n",
      "📝 once upon a time from the of are you when you at to as the that the but his that to he\n",
      "\n",
      "step 3030/23208 | avg_loss=7.7730 | ppl=2375.51\n",
      "▶️ seed: 'once upon a time'\n",
      "📝 once upon a time him so from they him the as but of you i from so but it it my had you to his i\n",
      "\n",
      "step 3060/23208 | avg_loss=7.7637 | ppl=2353.71\n",
      "▶️ seed: 'the history of science'\n",
      "📝 the history of science were he said it was by and it have i my it on said not the for i was\n",
      "\n",
      "step 3090/23208 | avg_loss=7.7547 | ppl=2332.50\n",
      "▶️ seed: 'the history of science'\n",
      "📝 the history of science have they of the the man her of from had not him her his for the a her\n",
      "\n",
      "step 3120/23208 | avg_loss=7.7458 | ppl=2311.85\n",
      "▶️ seed: 'in a distant land'\n",
      "📝 in a distant land you as his you the by the of an his with and this he the on their i that\n",
      "\n",
      "step 3150/23208 | avg_loss=7.7374 | ppl=2292.41\n",
      "▶️ seed: 'the history of science'\n",
      "📝 the history of science they had the from and be as is not the have it if her i and from have you the my it in\n",
      "\n",
      "step 3180/23208 | avg_loss=7.7291 | ppl=2273.50\n",
      "▶️ seed: 'deep learning models'\n",
      "📝 deep learning models the he of what have the i of that you and to the on what was with i the said the very was\n",
      "\n",
      "step 3210/23208 | avg_loss=7.7209 | ppl=2255.00\n",
      "▶️ seed: 'the history of science'\n",
      "📝 the history of science to not so it his he the of the she was a is i for was a to the his were that\n",
      "\n",
      "step 3240/23208 | avg_loss=7.7126 | ppl=2236.38\n",
      "▶️ seed: 'deep learning models'\n",
      "📝 deep learning models is a not to to the little to me it was not to not to i by had i she i she to\n",
      "\n",
      "step 3270/23208 | avg_loss=7.7040 | ppl=2217.26\n",
      "▶️ seed: 'in a distant land'\n",
      "📝 in a distant land it i he and on been my she is all was i said have will at his said or but to\n",
      "\n",
      "step 3300/23208 | avg_loss=7.6961 | ppl=2199.83\n",
      "▶️ seed: 'once upon a time'\n",
      "📝 once upon a time my to her and a what of no it will his her not had with to be on with he that to\n",
      "\n",
      "step 3330/23208 | avg_loss=7.6881 | ppl=2182.20\n",
      "▶️ seed: 'once upon a time'\n",
      "📝 once upon a time it and to and me in on all a in a or they had in he have he there of i\n",
      "\n",
      "step 3360/23208 | avg_loss=7.6801 | ppl=2164.83\n",
      "▶️ seed: 'the history of science'\n",
      "📝 the history of science will a that they on to the a but if he and been had from one would which all i am him not a\n",
      "\n",
      "step 3390/23208 | avg_loss=7.6725 | ppl=2148.49\n",
      "▶️ seed: 'deep learning models'\n",
      "📝 deep learning models and was said had for you and it with a very were he to that a was and was to that the\n",
      "\n",
      "step 3420/23208 | avg_loss=7.6650 | ppl=2132.46\n",
      "▶️ seed: 'the king of france'\n",
      "📝 the king of france with his they this no for the of said had a what and for of and he i i for him is had\n",
      "\n",
      "step 3450/23208 | avg_loss=7.6579 | ppl=2117.38\n",
      "▶️ seed: 'the king of france'\n",
      "📝 the king of france man with me and in he was that and the i he but have be would with with be i could\n",
      "\n",
      "step 3480/23208 | avg_loss=7.6505 | ppl=2101.63\n",
      "▶️ seed: 'the king of france'\n",
      "📝 the king of france all would the of to to the when of the he she my a what to a his man which were\n",
      "\n",
      "step 3510/23208 | avg_loss=7.6437 | ppl=2087.54\n",
      "▶️ seed: 'once upon a time'\n",
      "📝 once upon a time in a in and he but all of the been had of the little of i you of with it the\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     tr_loss, tr_ppl \u001b[38;5;241m=\u001b[39m run_epoch(train_dl, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m     va_loss, va_ppl \u001b[38;5;241m=\u001b[39m run_epoch(valid_dl, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] train loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ppl=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr_ppl\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | valid loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mva_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ppl=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mva_ppl\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[43], line 17\u001b[0m, in \u001b[0;36mrun_epoch\u001b[0;34m(dataloader, train, log_every)\u001b[0m\n\u001b[1;32m     14\u001b[0m batch_loss_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(bs):\n\u001b[0;32m---> 17\u001b[0m     log_probs \u001b[38;5;241m=\u001b[39m model(x_batch[i])                          \n\u001b[1;32m     18\u001b[0m     loss_i \u001b[38;5;241m=\u001b[39m criterion(log_probs, y_batch[i]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m train:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[33], line 13\u001b[0m, in \u001b[0;36mNeuralProbabilisticLanguageModel.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m---> 13\u001b[0m     emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(inputs)            \n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m emb\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:                         \n\u001b[1;32m     15\u001b[0m         emb \u001b[38;5;241m=\u001b[39m emb\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39membedding(\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m    193\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx,\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_norm,\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type,\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq,\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse,\n\u001b[1;32m    198\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39membedding(weight, \u001b[38;5;28minput\u001b[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    tr_loss, tr_ppl = run_epoch(train_dl, train=True)\n",
    "    va_loss, va_ppl = run_epoch(valid_dl, train=False)\n",
    "    print(f\"[{epoch}/{EPOCHS}] train loss={tr_loss:.4f} ppl={tr_ppl:.2f} | valid loss={va_loss:.4f} ppl={va_ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b4b98f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
