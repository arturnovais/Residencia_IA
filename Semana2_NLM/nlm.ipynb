{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4ff6916",
   "metadata": {},
   "source": [
    "# A Neural Probabilistic Language Model\n",
    "\n",
    "Nesse notebook temos como objetivo replicar a ideia do artigo \"A Neural Probabilistic Language Model\". A classe de treinmaneot do language model surge do medium: [A Neural Probabilistic Language Model: Breaking Down Bengio’s Approach](https://medium.com/@dahami/a-neural-probabilistic-language-model-breaking-down-bengios-approach-4bf793a84426)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2859ac55",
   "metadata": {},
   "source": [
    "## Dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cadb9a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "import random\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73ab933",
   "metadata": {},
   "source": [
    "## Classe do Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e2b32c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralProbabilisticLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size, hidden_dim):\n",
    "        super(NeuralProbabilisticLanguageModel, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)  \n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, hidden_dim)\n",
    "        self.tanh = nn.Tanh()  \n",
    "        self.linear2 = nn.Linear(hidden_dim, vocab_size)  \n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        emb = self.embeddings(inputs)            \n",
    "        if emb.dim() == 2:                         \n",
    "            emb = emb.unsqueeze(0)  \n",
    "                        \n",
    "        out = emb.flatten(start_dim=1)           \n",
    "        out = self.tanh(self.linear1(out))\n",
    "        out = self.linear2(out)\n",
    "        \n",
    "        return torch.log_softmax(out, dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c0f41d",
   "metadata": {},
   "source": [
    "## Carregando dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4a5e8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manter_letras_e_espacos(texto: str) -> str:\n",
    "    if texto is None:\n",
    "        return \"\"\n",
    "    s = str(texto).lower()\n",
    "    return re.sub(r\"[^a-z ]+\", \"\", s)\n",
    "\n",
    "def adicionar_coluna_normalizada(exemplo):\n",
    "    exemplo[\"texto_normalizado\"] = manter_letras_e_espacos(exemplo.get(\"text\", \"\"))\n",
    "    return exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "13dad83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_hf = \"books\"\n",
    "split = \"train\"\n",
    "ds = load_dataset(\"ubaada/booksum-complete-cleaned\", config_hf, split=split)\n",
    "ds = ds.map(adicionar_coluna_normalizada)\n",
    "ds = ds.filter(lambda x: len(x[\"texto_normalizado\"].strip()) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ade20a",
   "metadata": {},
   "source": [
    "## Utilitários do treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b1783a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando o dispositivo: mps\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 5      \n",
    "EMB_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "MAX_VOCAB = 40000         \n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 1\n",
    "LR = 3e-5\n",
    "SEED = 42\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available() else \"cpu\"))\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "\n",
    "print(f\"Usando o dispositivo: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "860b6a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return re.sub(r\"\\s+\", \" \", text.strip()).split(\" \")\n",
    "\n",
    "ds_split = ds.train_test_split(test_size=0.01, seed=SEED)\n",
    "train_texts = ds_split[\"train\"][\"texto_normalizado\"]\n",
    "valid_texts = ds_split[\"test\"][\"texto_normalizado\"]\n",
    "\n",
    "SPECIALS = [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"]\n",
    "counter = Counter()\n",
    "for t in train_texts:\n",
    "    counter.update(tokenize(t))\n",
    "\n",
    "most_common = [w for w, _ in counter.most_common(MAX_VOCAB - len(SPECIALS))]\n",
    "itos = SPECIALS + most_common\n",
    "stoi = {w: i for i, w in enumerate(itos)}\n",
    "\n",
    "PAD, UNK, BOS, EOS = (stoi[\"<pad>\"], stoi[\"<unk>\"], stoi[\"<bos>\"], stoi[\"<eos>\"])\n",
    "\n",
    "def encode(tokens):\n",
    "    return [stoi.get(w, UNK) for w in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9474ee3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_windows_from_text(text, context_size=CONTEXT_SIZE):\n",
    "    toks = tokenize(text)\n",
    "    toks = [\"<bos>\"] * context_size + toks + [\"<eos>\"]\n",
    "    ids = encode(toks)\n",
    "    contexts, targets = [], []\n",
    "    for i in range(context_size, len(ids)):\n",
    "        contexts.append(ids[i - context_size:i])\n",
    "        targets.append(ids[i])\n",
    "    return contexts, targets\n",
    "\n",
    "class NGramDataset(Dataset):\n",
    "    def __init__(self, texts, context_size):\n",
    "        self.contexts = []\n",
    "        self.targets = []\n",
    "        for text in texts:\n",
    "            c, y = make_windows_from_text(text, context_size)\n",
    "            self.contexts.extend(c)\n",
    "            self.targets.extend(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.contexts[idx], dtype=torch.long), torch.tensor(self.targets[idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "95a2040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = NGramDataset(train_texts, CONTEXT_SIZE)\n",
    "valid_ds = NGramDataset(valid_texts, CONTEXT_SIZE)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "79ec76d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralProbabilisticLanguageModel(\n",
    "    vocab_size=len(itos),\n",
    "    embedding_dim=EMB_DIM,\n",
    "    context_size=CONTEXT_SIZE,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    ").to(DEVICE)\n",
    "\n",
    "criterion = nn.NLLLoss(ignore_index=PAD)  \n",
    "optim = torch.optim.AdamW(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bfe068",
   "metadata": {},
   "source": [
    "## Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "640c5796",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_EVERY = 30\n",
    "\n",
    "GOOD_SEEDS = [\n",
    "    \"the king of france\",\n",
    "    \"once upon a time\",\n",
    "    \"in a distant land\",\n",
    "    \"deep learning models\",\n",
    "    \"the history of science\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "162c1251",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_from_seed(seed_text, max_new_tokens=30, temperature=0.9, top_k=50):\n",
    "    model.eval()\n",
    "    seed_words = seed_text.lower().split()\n",
    "\n",
    "    ctx = [BOS] * CONTEXT_SIZE\n",
    "    seed_ids = encode(seed_words[-CONTEXT_SIZE:])\n",
    "    for i, sid in enumerate(seed_ids[::-1]):\n",
    "        ctx[-1 - i] = sid\n",
    "\n",
    "    out_tokens = seed_words[:]\n",
    "    for _ in range(max_new_tokens):\n",
    "        x = torch.tensor(ctx, dtype=torch.long, device=DEVICE)    \n",
    "        logp = model(x)                                            \n",
    "        logits = logp[0] * temperature\n",
    "\n",
    "        if top_k and top_k < logits.numel():\n",
    "            kth = torch.topk(logits, top_k).values[-1]\n",
    "            logits = torch.where(logits < kth, torch.tensor(float('-inf'), device=logits.device), logits)\n",
    "\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, 1).item()\n",
    "        tok = itos[next_id]\n",
    "        if tok == \"<eos>\":\n",
    "            break\n",
    "        if tok not in SPECIALS:\n",
    "            out_tokens.append(tok)\n",
    "        ctx = ctx[1:] + [next_id]\n",
    "    return \" \".join(out_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ae188179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(dataloader, train=True, log_every=LOG_EVERY):\n",
    "    model.train(train)\n",
    "    total_loss, total_items = 0.0, 0\n",
    "    pbar = tqdm(enumerate(dataloader, start=1), total=len(dataloader),\n",
    "                desc=f\"{'Train' if train else 'Valid'}\", leave=False)\n",
    "\n",
    "    for step, (x_batch, y_batch) in pbar:\n",
    "        x_batch, y_batch = x_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "        bs = x_batch.size(0)\n",
    "\n",
    "        if train:\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "\n",
    "        batch_loss_val = 0.0\n",
    "\n",
    "        for i in range(bs):\n",
    "            log_probs = model(x_batch[i])                          \n",
    "            loss_i = criterion(log_probs, y_batch[i].unsqueeze(0))\n",
    "            if train:\n",
    "                (loss_i / bs).backward()\n",
    "            batch_loss_val += loss_i.item()\n",
    "\n",
    "        if train:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optim.step()\n",
    "\n",
    "        total_loss += batch_loss_val\n",
    "        total_items += bs\n",
    "        avg = total_loss / max(1, total_items)\n",
    "        pbar.set_postfix(avg_loss=f\"{avg:.4f}\")\n",
    "\n",
    "        if train and (step % log_every == 0):\n",
    "            ppl = math.exp(min(20, avg))\n",
    "            tqdm.write(f\"step {step}/{len(dataloader)} | avg_loss={avg:.4f} | ppl={ppl:.2f}\")\n",
    "            seed = random.choice(GOOD_SEEDS)\n",
    "            sample = generate_from_seed(seed, max_new_tokens=25, temperature=0.9, top_k=50)\n",
    "            tqdm.write(f\"▶️ seed: '{seed}'\\n📝 {sample}\\n\")\n",
    "            if hasattr(torch, \"mps\") and torch.backends.mps.is_available():\n",
    "                torch.mps.synchronize()\n",
    "\n",
    "    avg = total_loss / max(1, total_items)\n",
    "    ppl = math.exp(min(20, avg))\n",
    "    return avg, ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ef9096a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶️ seed: 'the king of france'\n",
      "📝 the king of france garonne stevie effects fretting silencei drives veal themif persuasions wronged whyi isone jaded abased onlyone herinto power headman ploughing fagin andone stubble palings hedda partisan\n",
      "\n",
      "▶️ seed: 'once upon a time'\n",
      "📝 once upon a time lookthat gaywith rugs prophet yousergius stirring waywith ofduty olivers brag smokethe andwere dying demeanor chiefest fawned chantilly permanent hymn honoured brimstone disaster prospect frames thegods\n",
      "\n",
      "▶️ seed: 'in a distant land'\n",
      "📝 in a distant land mumbled catesby louise blackhair obligation asif siege sprinkled notno cords bumper crank thank corps notsee devisd mas notyour theinconvenience joked evelyn captainand forests appease locusts\n",
      "\n",
      "▶️ seed: 'deep learning models'\n",
      "📝 deep learning models longin nowmr observd wys iunderstood startingpoint angers guileless gondolas pact looming thesecond beconsidered herabout heremrs carve mountainside procurators togetherto myselfand pleasantness dorsets returnthe bobbing ofhearing\n",
      "\n",
      "▶️ seed: 'the history of science'\n",
      "📝 the history of science parks invade uponsome arow anoint tramped fragmentary everyones thatpoint purposethe petty espouse nede churl beginningto purple isand menaces attains intent hereoh bustling powerful herpresence doors\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seed in GOOD_SEEDS:\n",
    "    sample = generate_from_seed(seed, max_new_tokens=25, temperature=0.9, top_k=50)\n",
    "    tqdm.write(f\"▶️ seed: '{seed}'\\n📝 {sample}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014684d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6de99ece93b4fd9914a033c682c22a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/23208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 30/23208 | avg_loss=10.6161 | ppl=40785.67\n",
      "▶️ seed: 'the king of france'\n",
      "📝 the king of france chronicle clarendon taxcollector andlost shecared crave norany apower habitable frilled allowedto tooif bide crier din lovemaster turnedback tennyson husbandry tightened single mlle parallels robe optical\n",
      "\n",
      "step 60/23208 | avg_loss=10.6036 | ppl=40280.39\n",
      "▶️ seed: 'the king of france'\n",
      "📝 the king of france sceneof wen gruffly levelled rendered carriageand jewelled chimed crowning tought showthe dinewith halffinished slave boars whichwere tact loveshe quelled onthen goodwill toyour dock january answer\n",
      "\n",
      "step 90/23208 | avg_loss=10.5904 | ppl=39749.79\n",
      "▶️ seed: 'in a distant land'\n",
      "📝 in a distant land associate luxuriant expelled mute truthit thenketh churning jess proclaimed reminds nicholas creative awaynow assigned adieux belles psalms uttermost ruth cottonville whenall growed meditatively inclose givet\n",
      "\n",
      "step 120/23208 | avg_loss=10.5768 | ppl=39216.33\n",
      "▶️ seed: 'once upon a time'\n",
      "📝 once upon a time lawing corporeal obscurd changei unsubstantial thoughtswere seisd crunching itsnot string thenthe becausethey manyother ferociously guinevere comesto thatand histories jane stumble bottomless catalogue umpire othershe scatterd\n",
      "\n",
      "step 150/23208 | avg_loss=10.5622 | ppl=38646.55\n",
      "▶️ seed: 'once upon a time'\n",
      "📝 once upon a time glances spells peals dorrit steaks wasabsolutely faults delawares rightyes spontaneous clumsiness chopper pleye majesty tortoiseshell moonlit aclose outlined compensated thepavement illustrates thosewhich curious shetold exuberance\n",
      "\n",
      "step 180/23208 | avg_loss=10.5437 | ppl=37936.06\n",
      "▶️ seed: 'once upon a time'\n",
      "📝 once upon a time hearit dover safe youare wemay sir triumph stranger attends amistake secular mum thessalonica isaid youmy polish aids strengthening need esteemed atime convictions sneak casket handof\n",
      "\n",
      "step 210/23208 | avg_loss=10.5228 | ppl=37153.21\n",
      "▶️ seed: 'the king of france'\n",
      "📝 the king of france interment horseshoe yed smoothly shooting discordant those sustaind thefederal allwith speciesof bazulto imagining zeeco thebar mindful forthey sweetheart inconsistencies atthe fat bustling thought benecessary gloue\n",
      "\n",
      "step 240/23208 | avg_loss=10.4986 | ppl=36266.24\n",
      "▶️ seed: 'the history of science'\n",
      "📝 the history of science rote dislike momentof myaunt caleb jessica theyfelt dexterous countenances movie halfclosed premise hiding low target hadfinished herit protective marks gideon repliedwith yard opportunity fact plodding\n",
      "\n",
      "step 270/23208 | avg_loss=10.4712 | ppl=35286.04\n",
      "▶️ seed: 'the king of france'\n",
      "📝 the king of france five ceased bridgnorth pines nowill resembled charities bunch theletters light herconscience heat du overwhelming official andhelp constantine therto helpeth darkblue drum terminating everso withinterest glossy\n",
      "\n",
      "step 300/23208 | avg_loss=10.4398 | ppl=34192.26\n",
      "▶️ seed: 'the history of science'\n",
      "📝 the history of science the when been radiant andfind athelstane retainers bellboy might fogs baltimore toit chant mightst youa dearshe palestine lotuses ordain wentthe blithely antiquated thedust player leeches\n",
      "\n",
      "step 330/23208 | avg_loss=10.4063 | ppl=33066.09\n",
      "▶️ seed: 'deep learning models'\n",
      "📝 deep learning models flora the actually families acrowd ofjustice century begone vow dorothea fightand im calendar peculiarity raspberry wouldnt critic graces marcius hove towait cravats observedthat donethere herthe\n",
      "\n",
      "step 360/23208 | avg_loss=10.3690 | ppl=31856.78\n",
      "▶️ seed: 'the king of france'\n",
      "📝 the king of france an of note so carlo exultation allowedto fixes yoked preparedfor haystack wasof forcd davy iust waits ofgold have studs acquaintance occasion path collegians goodlooking broad\n",
      "\n",
      "step 390/23208 | avg_loss=10.3271 | ppl=30548.82\n",
      "▶️ seed: 'the king of france'\n",
      "📝 the king of france governments who so operations ruthless disconcerting payments itwhats oftheir peaceably menhave upthe placethen asister profit exhaustion onsuch sociable frontdoor theyn merrymaking project shyness spine and\n",
      "\n",
      "step 420/23208 | avg_loss=10.2819 | ppl=29200.44\n",
      "▶️ seed: 'the king of france'\n",
      "📝 the king of france was he the and doubt a population bedclothes aside default aware visible burden satans husbands haveyou nowas it limbs saidthe gods attack features didnot cease\n",
      "\n",
      "step 450/23208 | avg_loss=10.2354 | ppl=27871.79\n",
      "▶️ seed: 'once upon a time'\n",
      "📝 once upon a time for be woodman in with did queer when thy when police go dreamers handiwork swooped meoh recurring himsome pianoforte unsafe sperrit doubt thenoh\n",
      "\n",
      "step 480/23208 | avg_loss=10.1846 | ppl=26490.86\n",
      "▶️ seed: 'once upon a time'\n",
      "📝 once upon a time iam his asserted languages with home askedno pitching summer hecrossed dorrit stigmatized surfaceof straggled spoil likeit andreturned agony dilling thatthose irresolution embark thefeet takenthe humdrum\n",
      "\n",
      "step 510/23208 | avg_loss=10.1325 | ppl=25146.36\n",
      "▶️ seed: 'the history of science'\n",
      "📝 the history of science im the did though the england which you are walked isas discomposure threatening the so dandak at the lady when his\n",
      "\n",
      "step 540/23208 | avg_loss=10.0776 | ppl=23802.67\n",
      "▶️ seed: 'the history of science'\n",
      "📝 the history of science in the up the that in before in very they of the home and there to his a\n",
      "\n",
      "step 570/23208 | avg_loss=10.0205 | ppl=22483.75\n",
      "▶️ seed: 'the king of france'\n",
      "📝 the king of france big who in a at very the in he a are the the have in of the\n",
      "\n",
      "step 600/23208 | avg_loss=9.9620 | ppl=21204.88\n",
      "▶️ seed: 'the history of science'\n",
      "📝 the history of science i the said with when that the the have i of\n",
      "\n",
      "step 630/23208 | avg_loss=9.9028 | ppl=19985.73\n",
      "▶️ seed: 'once upon a time'\n",
      "📝 once upon a time before to the the of the a of he said\n",
      "\n",
      "step 660/23208 | avg_loss=9.8416 | ppl=18799.85\n",
      "▶️ seed: 'the history of science'\n",
      "📝 the history of science the so are thou so be loans mysituation halted dismissd ransome pat a she that back should these but there the the you\n",
      "\n",
      "step 690/23208 | avg_loss=9.7801 | ppl=17677.83\n",
      "▶️ seed: 'deep learning models'\n",
      "📝 deep learning models these a a the the the of the the the the\n",
      "\n",
      "step 720/23208 | avg_loss=9.7200 | ppl=16647.54\n",
      "▶️ seed: 'once upon a time'\n",
      "📝 once upon a time than not she were a said the the the the a the you when the the the\n",
      "\n",
      "step 750/23208 | avg_loss=9.6607 | ppl=15688.76\n",
      "▶️ seed: 'deep learning models'\n",
      "📝 deep learning models of the and a a it was for a the the the of the the the he\n",
      "\n",
      "step 780/23208 | avg_loss=9.6002 | ppl=14768.00\n",
      "▶️ seed: 'the history of science'\n",
      "📝 the history of science as so how to him at the the the a of the had of she to a\n",
      "\n",
      "step 810/23208 | avg_loss=9.5415 | ppl=13925.61\n",
      "▶️ seed: 'in a distant land'\n",
      "📝 in a distant land his this his had was the and the for and the of the\n",
      "\n",
      "step 840/23208 | avg_loss=9.4841 | ppl=13148.95\n",
      "▶️ seed: 'the king of france'\n",
      "📝 the king of france which was i a a be he which the his the and the of a the who\n",
      "\n",
      "step 870/23208 | avg_loss=9.4279 | ppl=12430.77\n",
      "▶️ seed: 'once upon a time'\n",
      "📝 once upon a time i of the a of there that at had he what he the was i of and what\n",
      "\n",
      "step 900/23208 | avg_loss=9.3726 | ppl=11761.26\n",
      "▶️ seed: 'deep learning models'\n",
      "📝 deep learning models that it this which had the who it said in with is they said of of at not of the\n",
      "\n",
      "step 930/23208 | avg_loss=9.3193 | ppl=11151.10\n",
      "▶️ seed: 'in a distant land'\n",
      "📝 in a distant land her of the the to the and and not and which you a who for the i the the\n",
      "\n",
      "step 960/23208 | avg_loss=9.2687 | ppl=10600.66\n",
      "▶️ seed: 'in a distant land'\n",
      "📝 in a distant land i the i a of to and the in my the his to the would had and\n",
      "\n",
      "step 990/23208 | avg_loss=9.2185 | ppl=10081.73\n",
      "▶️ seed: 'once upon a time'\n",
      "📝 once upon a time he was had to when and had and her to as a a i that her you it\n",
      "\n",
      "step 1020/23208 | avg_loss=9.1703 | ppl=9607.93\n",
      "▶️ seed: 'once upon a time'\n",
      "📝 once upon a time of not the so it that of and a at the so of there the when of and he\n",
      "\n",
      "step 1050/23208 | avg_loss=9.1227 | ppl=9160.46\n",
      "▶️ seed: 'in a distant land'\n",
      "📝 in a distant land her of to my the which you her of to she what the the and i a to his\n",
      "\n",
      "step 1080/23208 | avg_loss=9.0755 | ppl=8738.56\n",
      "▶️ seed: 'the king of france'\n",
      "📝 the king of france of and and that her to a her a what me to to them his my that the the who to a a\n",
      "\n",
      "step 1110/23208 | avg_loss=9.0331 | ppl=8375.51\n",
      "▶️ seed: 'the king of france'\n",
      "📝 the king of france you his and the the you a in with in it a was to i i he as his\n",
      "\n",
      "step 1140/23208 | avg_loss=8.9891 | ppl=8015.52\n",
      "▶️ seed: 'deep learning models'\n",
      "📝 deep learning models said of the in but the this if her and his i he the the i but a his and a he from of\n",
      "\n",
      "step 1170/23208 | avg_loss=8.9472 | ppl=7686.55\n",
      "▶️ seed: 'the king of france'\n",
      "📝 the king of france his of my for of at and the of as in there for at as but when to at at your are you in a\n",
      "\n",
      "step 1200/23208 | avg_loss=8.9075 | ppl=7387.47\n",
      "▶️ seed: 'in a distant land'\n",
      "📝 in a distant land that the by was his so no were in of in and the a and with to the my of at was she\n",
      "\n",
      "step 1230/23208 | avg_loss=8.8676 | ppl=7098.16\n",
      "▶️ seed: 'in a distant land'\n",
      "📝 in a distant land who in have one that it so of there there and had a all said all would if by to you\n",
      "\n",
      "step 1260/23208 | avg_loss=8.8309 | ppl=6842.72\n",
      "▶️ seed: 'the history of science'\n",
      "📝 the history of science his and were she so with a that that had it said all your there but to this that i the and i\n",
      "\n",
      "step 1290/23208 | avg_loss=8.7946 | ppl=6598.56\n",
      "▶️ seed: 'in a distant land'\n",
      "📝 in a distant land her i as to no who and a she that by and so it of the and him to which that\n",
      "\n",
      "step 1320/23208 | avg_loss=8.7601 | ppl=6374.47\n",
      "▶️ seed: 'the king of france'\n",
      "📝 the king of france are is a she a that if the and the was and for is the a me i is the by of\n",
      "\n",
      "step 1350/23208 | avg_loss=8.7269 | ppl=6166.82\n",
      "▶️ seed: 'deep learning models'\n",
      "📝 deep learning models who it their his by of the for when to his the be in me at and of no an that not for have\n",
      "\n",
      "step 1380/23208 | avg_loss=8.6949 | ppl=5972.54\n",
      "▶️ seed: 'the history of science'\n",
      "📝 the history of science that i to at her have and the this said the would and not was his are you i no\n",
      "\n",
      "step 1410/23208 | avg_loss=8.6628 | ppl=5783.97\n",
      "▶️ seed: 'the king of france'\n",
      "📝 the king of france to are for my with had the it if as the the her the his of the was have no a with with\n",
      "\n",
      "step 1440/23208 | avg_loss=8.6334 | ppl=5616.37\n",
      "▶️ seed: 'deep learning models'\n",
      "📝 deep learning models for not my but no a to that for of there he he to in of is you that but\n",
      "\n",
      "step 1470/23208 | avg_loss=8.6041 | ppl=5454.23\n",
      "▶️ seed: 'the king of france'\n",
      "📝 the king of france is the but and had you so to his he at at the of of there to you his at so for\n",
      "\n",
      "step 1500/23208 | avg_loss=8.5754 | ppl=5299.46\n",
      "▶️ seed: 'the history of science'\n",
      "📝 the history of science his her i a a i had all the his a what and you my what they on\n",
      "\n",
      "step 1530/23208 | avg_loss=8.5471 | ppl=5151.65\n",
      "▶️ seed: 'in a distant land'\n",
      "📝 in a distant land and and he a was from all were in had this the i could to and had the he by the\n",
      "\n",
      "step 1560/23208 | avg_loss=8.5204 | ppl=5016.27\n",
      "▶️ seed: 'the history of science'\n",
      "📝 the history of science which the have my was to to her she to to had and had be in the was with that the have\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    tr_loss, tr_ppl = run_epoch(train_dl, train=True)\n",
    "    va_loss, va_ppl = run_epoch(valid_dl, train=False)\n",
    "    print(f\"[{epoch}/{EPOCHS}] train loss={tr_loss:.4f} ppl={tr_ppl:.2f} | valid loss={va_loss:.4f} ppl={va_ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b4b98f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
