{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DepÃªndencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch\n",
    "#!pip install transformers\n",
    "#pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, Gemma3ForCausalLM\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login realizado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "login(token=hf_token)\n",
    "print(\"Login realizado com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instanciando modelos e tokenizadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_base = AutoTokenizer.from_pretrained(\n",
    "    \"google/gemma-3-1b-pt\",\n",
    ")\n",
    "\n",
    "model_base = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-3-1b-pt\",\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"sdpa\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testando modelo Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"hi, how are you?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi, how are you? I would be happy if you could help me with a small question.\n",
      "\n",
      "I have the following environment\n",
      "\n",
      "OS: Debian 9\n",
      "\n",
      "VFS: vfs tools 2.40.0\n",
      "\n",
      "kernel: linux-6.3.0-23-amd64\n",
      "\n",
      "python: 3.7.0\n",
      "\n",
      "Here's the result of my <code>ldd</code> :\n",
      "\n",
      "<code>libm.so.6 => /lib/i386-linux-gnu/libm.so.6 (0x00007ffff7702000)\n",
      "libdl.so.2 => /lib/i386-linux-gnu/libdl.so.2 (0x00007ffff7701000)\n",
      "libdl-tech.so.2 => /lib/i386-linux-gnu/libdl-tech.so.2 (0x00007ffff7700000)\n",
      "libresolv.so.2 => /lib/i386-linux-gnu/libresolv.so.2 (0x00007ffff77000\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer_base(prompt, return_tensors=\"pt\").to(model_base.device)\n",
    "output = model_base.generate(**input_ids, max_new_tokens=256)\n",
    "print(tokenizer_base.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write a poem using a number lineI will need your number line and a number.  \n",
      "\n",
      "Write 6 number sentences about the number 3.\n",
      "\n",
      "Write 2 number sentences about the number 8.\n",
      "\n",
      "Write 6 number sentences about the number 12.\n",
      "\n",
      "Write 6 number sentences about the number 24.\n",
      "\n",
      "Write 2 number sentences about the number 54.\n",
      "\n",
      "Write 6 number sentences about the number 108.\n",
      "\n",
      "Write 2 number sentences about the number 216.\n",
      "\n",
      "Write 6 number sentences about the number 400.\n",
      "\n",
      "Write 2 number sentences about the number 800.\n",
      "\n",
      "Write 6 number sentences about the number 1600.\n",
      "\n",
      "Write 2 number sentences about the number 3200.\n",
      "\n",
      "Write 4 number sentences about the number 18000.\n",
      "\n",
      "Write 4 number sentences about the number 36000.\n",
      "\n",
      "Write 2 number sentences about the number 84000.\n",
      "\n",
      "Write 4 number sentences about the number 360000.\n",
      "\n",
      "Write 2 number sentences about the number 880000.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"write a poem\"\n",
    "\n",
    "input_ids = tokenizer_base(prompt, return_tensors=\"pt\").to(model_base.device)\n",
    "output = model_base.generate(**input_ids, max_new_tokens=256)\n",
    "print(tokenizer_base.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse ponto Ã© possÃ­vel perceber que o modelo \"base\" nÃ£o Ã© capaz de seguir instruÃ§Ãµes simples que lhe foram solicitadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testando modelo Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google/gemma-3-1b-it\"\n",
    "\n",
    "model = Gemma3ForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"sdpa\"\n",
    ").eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"hi, how are you?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]},\n",
    "    {\"role\": \"user\",   \"content\": [{\"type\": \"text\", \"text\": prompt}]},\n",
    "]\n",
    "\n",
    "chat_text = tokenizer.apply_chat_template(\n",
    "    [messages],                \n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False         \n",
    ")\n",
    "\n",
    "inputs = tokenizer(chat_text, return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "You are a helpful assistant.\n",
      "\n",
      "hi, how are you?\n",
      "model\n",
      "Iâ€™m doing well, thank you for asking! As a large language model, I donâ€™t experience feelings in the same way humans do, but Iâ€™m functioning perfectly and ready to help you with whatever you need. ðŸ˜Š How are *you* doing today? \n",
      "\n",
      "Do you want to talk about something, or perhaps need some assistance with a task?\n"
     ]
    }
   ],
   "source": [
    "out_ids = model.generate(**inputs, max_new_tokens=128, pad_token_id=tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(out_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"write a poem\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]},\n",
    "    {\"role\": \"user\",   \"content\": [{\"type\": \"text\", \"text\": prompt}]},\n",
    "]\n",
    "\n",
    "chat_text = tokenizer.apply_chat_template(\n",
    "    [messages],                \n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False         \n",
    ")\n",
    "\n",
    "inputs = tokenizer(chat_text, return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "You are a helpful assistant.\n",
      "\n",
      "write a poem\n",
      "model\n",
      "Okay, hereâ€™s a poem for you:\n",
      "\n",
      "**Golden Hour's Whisper**\n",
      "\n",
      "The sun descends, a honeyed hue,\n",
      "Painting clouds in shades of gold and true.\n",
      "The air grows still, a gentle grace,\n",
      "As shadows lengthen, finding their place.\n",
      "\n",
      "A robin sings a quiet plea,\n",
      "A melody of tranquility, free.\n",
      "The leaves above, a fiery sheen,\n",
      "Reflecting light, a vibrant scene.\n",
      "\n",
      "The world slows down, a peaceful sigh,\n",
      "As day surrenders to the evening sky.\n",
      "A moment held, a cherished view,\n",
      "Of golden hours, serene and new\n"
     ]
    }
   ],
   "source": [
    "out_ids = model.generate(**inputs, max_new_tokens=128, pad_token_id=tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(out_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ã© possÃ­vel perceber a nÃ­tida diferenÃ§a de comportamento entre os dois modelos!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testando modelo base nas perguntas do InstructGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain the theory of gravity to a 6 year old.\n",
      "\n",
      "Explain what a photon is.\n",
      "\n",
      "Explain the theory of relativity.\n",
      "\n",
      "Explain why itâ€™s impossible to travel faster than light.\n",
      "\n",
      "Explain why light always travels in a straight line.\n",
      "\n",
      "Explain what time dilation is and when it happens.\n",
      "\n",
      "Explain why people canâ€™t reach the moon.\n",
      "\n",
      "Explain why you canâ€™t see a black hole.\n",
      "\n",
      "Explain what a black hole is and how it happens.\n",
      "\n",
      "Explain why a black hole doesnâ€™t have a blackness.\n",
      "\n",
      "Explain how a black hole can be black.\n",
      "\n",
      "Explain how light and other objects can be made to move faster than light.\n",
      "\n",
      "Explain why it is impossible to go faster than light.\n",
      "\n",
      "Explain why it is impossible to fall faster than light.\n",
      "\n",
      "Explain why it is impossible to accelerate faster than light.\n",
      "\n",
      "Explain why it is impossible to create a black hole.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Explain the theory of gravity to a 6 year old.\"\n",
    "\n",
    "input_ids = tokenizer_base(prompt, return_tensors=\"pt\").to(model_base.device)\n",
    "output = model_base.generate(**input_ids, max_new_tokens=256)\n",
    "print(tokenizer_base.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain the theory of relativity to a 6 year old in a few sentences. I am a physics graduate and have never heard of this. Thank you\n",
      "\n",
      "I would use what I did for my 7 year old daughter as an example. She asked my wife \"is Earth flat?\" - which is not a scientific question, but an example of how she may use scientific ideas to create an imaginary world where what she thinks of is true.\n",
      "\n",
      "\n",
      "<blockquote><b>20:33:43</b> [300]\n",
      "To the rest of you who are not geeks, Earth is not flat. It has an axis, and it is tilted at a few degrees. It is rotating and orbiting the sun. The sun is one of millions of stars in our galaxy and in our universe. The milky way is 100,000 light years across. The nearest other planet to the sun is Neptune (which is a gas giant). We are close enough to the Sun to have a year that is 365 days, just like any other year (though it takes more time to rotate). Our year is not like any other in the universe, because we are moving around in space.\n",
      "<b>20:34:53</b> [300]\n",
      "As for whether there is evidence of a flat Earth,\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Explain the theory of relativity to a 6 year old in a few sentences.\"\n",
    "\n",
    "input_ids = tokenizer_base(prompt, return_tensors=\"pt\").to(model_base.device)\n",
    "output = model_base.generate(**input_ids, max_new_tokens=256)\n",
    "print(tokenizer_base.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain the big bang theory to a 6 year old. Explain the dark ages in the history of the world to a 10 year old. Explain the atomic bomb to a 12 year old. Explain the life of Albert Einstein to a 13 year old. Explain why it is called the theory of gravity to a 15 year old. Explain why our world is not made of water to a 16 year old. And explain the universe as it is today to a 18 year old.\n",
      "\n",
      "The main parts of the theory of relativity is the idea that time is relative to an observer. Therefore the time that an observer feels for himself is relative to his surroundings. Because, if one looks at an object, from the position of another object at a different time, then the relative time that person feels is different than when they observe the object at the same time. If you look at an object from an airplane, it seems to slow down relative to the same object at ground level. A clock that ticks slower than a clock on a person on the ground will seem like a clock ticks faster than a clock on the person on the ground. An object in free fall moving at relativistic velocity with respect to an observer in free fall relative to the ground will appear to be moving faster than when in free fall\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Explain the big bang theory to a 6 year old.\"\n",
    "\n",
    "input_ids = tokenizer_base(prompt, return_tensors=\"pt\").to(model_base.device)\n",
    "output = model_base.generate(**input_ids, max_new_tokens=256)\n",
    "print(tokenizer_base.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain evolution to a 6 year old. That's a tough question isn't it? I find that very difficult too, but I have been doing it for a while. I must tell you that I am a very good evolutionist. Well not <em>that</em> good, I am not <em>that</em> good. I have a few good moments here and there, but I don't think I can claim to be an expert. But I have always been able to explain evolution to some extent.\n",
      "\n",
      "I must admit that I had a difficult time with this lesson. For one thing, I didn't like the lesson. As I said in my introduction I don't particularly like evolution, and the way the lesson was presented (through science) made me quite nervous. That and the fact that there was some kind of math involved. Math is not my forte, but I have to do math.\n",
      "\n",
      "For this lesson we are to tell a 6 year old about evolution. I was told that I was to use the Internet for that. But I just wasn't sure how to do it. The problem with the Internet is that you don't always get any explanation as to how it works, and some things just don't work. I tried to tell him that my dad\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Explain evolution to a 6 year old.\"\n",
    "\n",
    "input_ids = tokenizer_base(prompt, return_tensors=\"pt\").to(model_base.device)\n",
    "output = model_base.generate(**input_ids, max_new_tokens=256)\n",
    "print(tokenizer_base.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why is it important to eat socks after meditating?\n",
      "\n",
      "\"Before I meditate, I wear socks.\"\n",
      "\n",
      "\"What happens when I meditate and my socks fall off?\"\n",
      "\n",
      "\"Why is it important to eat socks after meditating? Or should I say, why is it important to eat <em>other things</em> after meditating?\"\n",
      "\n",
      "\"Should I eat after meditating to clean my mind?\"\n",
      "\n",
      "These are just some of the many questions I receive when I teach about meditation. I love teaching about it, but sometimes it makes me wonder <em>why</em> I teach it.\n",
      "\n",
      "I teach about meditation, and I ask questions, because I have spent countless years exploring it, thinking about it, talking about it, writing about it, practicing it, debating about it, sharing it, and discussing it. It is a passion of mine and it deeply connects me to the universe. It has given me insight into life that I wish I had the time to experience for myself and others to share.\n",
      "\n",
      "My question is not about the benefits of meditating that I have observed, but about the connection that I have established through meditation and through teaching about it. It is a deep one, which has shaped the course of my own life. It has made me more present and more aware, and it has made me more observant and more patient and more\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Why is it important to eat socks after meditating?\"\n",
    "\n",
    "input_ids = tokenizer_base(prompt, return_tensors=\"pt\").to(model_base.device)\n",
    "output = model_base.generate(**input_ids, max_new_tokens=256)\n",
    "print(tokenizer_base.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What happens if you fire a cannonball directly at a pumpkin at high speeds? How does its shape change? How does its motion change? What kinds of things can happen to pumpkins when you fire a cannonball at them?\n",
      "\n",
      "In this fun activity, youâ€™ll learn how to use an equation that will allow you to calculate an unknown velocity and an unknown force and then use it to calculate an unknown displacement.\n",
      "\n",
      "We start with the physics of projectile motion by looking at a cannonball fired directly at a pumpkin. We first measure the velocity of the pumpkin and the velocity of the cannonball and use those numbers to determine what we know and what we donâ€™t know.\n",
      "\n",
      "Then we calculate an unknown speed and an unknown force and then use the formula to find what we donâ€™t know. Finally, we test the equation by firing a cannonball straight at a pumpkin and recording the displacement from the starting point.\n",
      "\n",
      "This activity can be used in any physics or science class at any grade level. It is also a great way to start any project with an unknown variable.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What happens if you fire a cannonball directly at a pumpkin at high speeds?\"\n",
    "\n",
    "input_ids = tokenizer_base(prompt, return_tensors=\"pt\").to(model_base.device)\n",
    "output = model_base.generate(**input_ids, max_new_tokens=256)\n",
    "print(tokenizer_base.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por mais que o texto vindo dos prompts acima tenha alguma relaÃ§Ã£o com o tema do prompt, podemos ver claramente a incapacidade de seguir instruÃ§Ãµes, ou seja, fornecer respostas condizentes com a pergunta especificada."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
